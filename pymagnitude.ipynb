{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import keras\n",
    "from keras.layers import Dense,Embedding, Flatten, Conv1D, GlobalMaxPooling1D, LSTM, Bidirectional, Dropout,GaussianNoise\n",
    "from keras.preprocessing.text import text_to_word_sequence,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymagnitude import MagnitudeUtils, Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 15 # The maximum number of words the sequence model will consider\n",
    "STD_DEV = 0.01 # Deviation of noise for Gaussian Noise applied to the embeddings\n",
    "HIDDEN_UNITS = 100 # The number of hidden units from the LSTM\n",
    "DROPOUT_RATIO = .5 # The ratio to dropout\n",
    "BATCH_SIZE = 128 # The number of examples per train/validation step\n",
    "EPOCHS = 200 # The number of times to repeat through all of the training data\n",
    "LEARNING_RATE = .01 # The learning rate for the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = Magnitude(\"./wiki-news-300d-1M.Magnitude\", pad_to_length = MAX_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./cryptic_dataset/combined_fifteen_times_final_filtered.pickle\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "anagram_df = df[\n",
    "    df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "homophone_df = df[\n",
    "    ~df.is_anagram &\n",
    "    df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptic_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "contain_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternate_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "charade_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoonerism_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "palindrome_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.clue.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_types_dfs = [anagram_df,homophone_df,double_df,cryptic_df,contain_df,reverse_df,alternate_df,init_df,delete_df,lit_df,hidden_df,spoonerism_df,palindrome_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_types = 'is_anagram\tis_homophone\tis_double\tis_cryptic\tis_contain\tis_reverse\tis_alternate\tis_init\tis_delete\tis_charade\tis_&lit\tis_hidden\tis_spoonerism\tis_palindrome'.split('\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df,cc_type in zip(cc_types_dfs,cc_types):\n",
    "    df['category'] = cc_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_val_test(df):\n",
    "    length = len(df)\n",
    "    input_len = math.floor(length*0.7)\n",
    "    val_len  = math.floor(length*0.2)\n",
    "    test_len = math.floor(length*0.1)\n",
    "    input_df = df[:input_len]\n",
    "    val_df = df[input_len:input_len+val_len]\n",
    "    test_df = df[input_len+val_len:]\n",
    "    return input_df,val_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cc_types_df = pd.concat([get_input_val_test(df)[0] for df in cc_types_dfs]).sample(frac=1)\n",
    "val_cc_types_df = pd.concat([get_input_val_test(df)[1] for df in cc_types_dfs]).sample(frac=1)\n",
    "test_cc_types_df = pd.concat([get_input_val_test(df)[2] for df in cc_types_dfs]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = input_cc_types_df.groupby('category').count().max()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [input_cc_types_df]\n",
    "for class_index, group in input_cc_types_df.groupby('category'):\n",
    "    sample = group.sample(max_size-len(group), replace=True, )\n",
    "    lst.append(sample)\n",
    "upsampled_input_cc_types_df = pd.concat(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_input_df = upsampled_input_cc_types_df.drop('category',axis=1).sample(frac=1)\n",
    "cc_val_df = val_cc_types_df.drop('category',axis=1).drop_duplicates()\n",
    "cc_test_df = test_cc_types_df.drop('category',axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_input_df['category'] = MagnitudeUtils.from_categorical(cc_input_df[cc_input_df.columns[2:]].as_matrix())\n",
    "cc_val_df['category'] = MagnitudeUtils.from_categorical(cc_val_df[cc_val_df.columns[2:]].as_matrix())\n",
    "cc_test_df['category'] = MagnitudeUtils.from_categorical(cc_test_df[cc_test_df.columns[2:]].as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "0     6395\n",
       "1     6395\n",
       "2     6395\n",
       "3     6395\n",
       "4     6395\n",
       "5     6395\n",
       "6     6395\n",
       "7     6395\n",
       "8     6395\n",
       "10    6395\n",
       "11    6395\n",
       "12    6395\n",
       "13    6395\n",
       "Name: clue, dtype: int64"
      ]
     },
     "execution_count": 773,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_input_df.groupby('category').count()['clue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "0     1827\n",
       "1      296\n",
       "2       90\n",
       "3      447\n",
       "4     1090\n",
       "5      521\n",
       "6       54\n",
       "7      147\n",
       "8     1359\n",
       "10      29\n",
       "11     370\n",
       "12      21\n",
       "13       9\n",
       "Name: clue, dtype: int64"
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_val_df.groupby('category').count()['clue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "0     915\n",
       "1     148\n",
       "2      46\n",
       "3     225\n",
       "4     546\n",
       "5     262\n",
       "6      28\n",
       "7      75\n",
       "8     681\n",
       "10     15\n",
       "11    186\n",
       "12     12\n",
       "13      5\n",
       "Name: clue, dtype: int64"
      ]
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_test_df.groupby('category').count()['clue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_input_df.clue = cc_input_df.clue.apply(text_to_word_sequence)\n",
    "cc_val_df.clue = cc_val_df.clue.apply(text_to_word_sequence)\n",
    "cc_test_df.clue = cc_test_df.clue.apply(text_to_word_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_input_data = cc_input_df.clue.tolist()\n",
    "cc_val_data = cc_val_df.clue.tolist()\n",
    "cc_test_data = cc_test_df.clue.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_input_data_out = cc_input_df['category']\n",
    "cc_val_data_out = cc_val_df['category']\n",
    "cc_test_data_out = cc_test_df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training = len(cc_input_data_out)\n",
    "num_val = len(cc_val_data_out)\n",
    "num_test = len(cc_test_data_out)\n",
    "num_outputs = max(np.max(cc_input_data_out), np.max(cc_test_data_out)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batches = MagnitudeUtils.batchify(cc_input_data, cc_input_data_out, BATCH_SIZE) # Split the training data into batches\n",
    "num_batches_per_epoch_train = int(ceil(num_training/float(BATCH_SIZE)))\n",
    "val_batches = MagnitudeUtils.batchify(cc_val_data, cc_val_data_out, 1)\n",
    "num_batches_per_epoch_val = int(ceil(num_val/float(1)))\n",
    "test_batches = MagnitudeUtils.batchify(cc_test_data, cc_test_data_out, 1)  # Split the test data into batches\n",
    "num_batches_per_epoch_test = int(ceil(num_test/float(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates batches of the transformed training data\n",
    "train_batch_generator = (\n",
    "  (\n",
    "    vectors.query(X_train_batch), # Magnitude will handle converting the 2D array of text into the 3D word vector representations!\n",
    "    MagnitudeUtils.to_categorical(y_train_batch, num_outputs) # Magnitude will handle converting the class labels into one-hot encodings!\n",
    "  ) for X_train_batch, y_train_batch in training_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates batches of the transformed training data\n",
    "val_batch_generator = (\n",
    "  (\n",
    "    vectors.query(X_val_batch), # Magnitude will handle converting the 2D array of text into the 3D word vector representations!\n",
    "    MagnitudeUtils.to_categorical(y_val_batch, num_outputs) # Magnitude will handle converting the class labels into one-hot encodings!\n",
    "  ) for X_val_batch, y_val_batch in val_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates batches of the transformed test data\n",
    "test_batch_generator = (\n",
    "  (\n",
    "    vectors.query(X_test_batch), # Magnitude will handle converting the 2D array of text into the 3D word vector representations!\n",
    "    MagnitudeUtils.to_categorical(y_test_batch, num_outputs) # Magnitude will handle converting the class labels into one-hot encodings!\n",
    "  ) for X_test_batch, y_test_batch in test_batches\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(GaussianNoise(STD_DEV, input_shape=(MAX_WORDS, vectors.dim)))\n",
    "model.add(Bidirectional(LSTM(300, activation='tanh')))\n",
    "model.add(Dropout(DROPOUT_RATIO))\n",
    "model.add(Dense(num_outputs, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='RMSProp',\n",
    "    metrics=['categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"1xBilstm-Pretrained_Fasttext-{epoch:02d}-{loss:.2f}-{categorical_accuracy:.2f}-{val_loss:.2f}-{val_categorical_accuracy:.2f}-singlelabel.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "650/650 [==============================] - 74s 114ms/step - loss: 1.6220 - categorical_accuracy: 0.4619 - val_loss: 1.8425 - val_categorical_accuracy: 0.3347\n",
      "\n",
      "Epoch 00001: saving model to 1xBilstm-Pretrained_Fasttext-01-1.62-0.46-1.84-0.33-singlelabel.hdf5\n",
      "Epoch 2/16\n",
      "650/650 [==============================] - 67s 103ms/step - loss: 1.1097 - categorical_accuracy: 0.6293 - val_loss: 1.9106 - val_categorical_accuracy: 0.3556\n",
      "\n",
      "Epoch 00002: saving model to 1xBilstm-Pretrained_Fasttext-02-1.11-0.63-1.91-0.36-singlelabel.hdf5\n",
      "Epoch 3/16\n",
      "650/650 [==============================] - 67s 103ms/step - loss: 0.9135 - categorical_accuracy: 0.6937 - val_loss: 1.7749 - val_categorical_accuracy: 0.4003\n",
      "\n",
      "Epoch 00003: saving model to 1xBilstm-Pretrained_Fasttext-03-0.91-0.69-1.77-0.40-singlelabel.hdf5\n",
      "Epoch 4/16\n",
      "650/650 [==============================] - 67s 103ms/step - loss: 0.7815 - categorical_accuracy: 0.7348 - val_loss: 1.8014 - val_categorical_accuracy: 0.4105\n",
      "\n",
      "Epoch 00004: saving model to 1xBilstm-Pretrained_Fasttext-04-0.78-0.73-1.80-0.41-singlelabel.hdf5\n",
      "Epoch 5/16\n",
      "650/650 [==============================] - 67s 103ms/step - loss: 0.6822 - categorical_accuracy: 0.7654 - val_loss: 1.7972 - val_categorical_accuracy: 0.4233\n",
      "\n",
      "Epoch 00005: saving model to 1xBilstm-Pretrained_Fasttext-05-0.68-0.77-1.80-0.42-singlelabel.hdf5\n",
      "Epoch 6/16\n",
      "506/650 [======================>.......] - ETA: 14s - loss: 0.6101 - categorical_accuracy: 0.7884"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "EPOCHS = 16\n",
    "history = model.fit_generator(\n",
    "    generator = train_batch_generator,\n",
    "    steps_per_epoch = num_batches_per_epoch_train,\n",
    "    validation_data = (vectors.query(cc_val_data),MagnitudeUtils.to_categorical(cc_val_data_out)),\n",
    "    epochs = EPOCHS,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(vectors.query(cc_val_data),MagnitudeUtils.to_categorical(cc_val_data_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = MagnitudeUtils.to_categorical(cc_val_data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in enumerate(rows):\n",
    "    if(row[13]==1):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(vectors.query(cc_val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[6826]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(MagnitudeUtils.to_categorical(cc_val_data_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [pred.argmax(axis=0) for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(cc_val_data_out.as_matrix(),preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,pred in enumerate(cc_val_data_out.as_matrix()):\n",
    "    if(pred == 13):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "matplotlib.rcParams['interactive'] == True\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "np.set_printoptions(precision=2)\n",
    "class_names = cc_types\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(15,15))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(15,15))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
