{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import keras\n",
    "from keras.layers import Dense,Embedding, Flatten, Conv1D, GlobalMaxPooling1D, LSTM, Bidirectional, Dropout\n",
    "from keras.preprocessing.text import text_to_word_sequence,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./cryptic_dataset/combined_fifteen_times_final_filtered.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "anagram_df = df[\n",
    "    df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "homophone_df = df[\n",
    "    ~df.is_anagram &\n",
    "    df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptic_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "contain_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternate_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "charade_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoonerism_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    df.is_spoonerism & \n",
    "    ~df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "palindrome_df = df[\n",
    "    ~df.is_anagram &\n",
    "    ~df.is_homophone &\n",
    "    ~df.is_double &\n",
    "    ~df.is_cryptic & \n",
    "    ~df.is_contain & \n",
    "    ~df.is_reverse & \n",
    "    ~df.is_alternate &\n",
    "    ~df.is_init & \n",
    "    ~df.is_delete & \n",
    "    ~df.is_charade & \n",
    "    ~df['is_&lit'] & \n",
    "    ~df.is_hidden & \n",
    "    ~df.is_spoonerism & \n",
    "    df.is_palindrome\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.clue.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_types_dfs = [anagram_df,homophone_df,double_df,cryptic_df,contain_df,reverse_df,alternate_df,init_df,delete_df,charade_df,lit_df,hidden_df,spoonerism_df,palindrome_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_types = 'is_anagram\tis_homophone\tis_double\tis_cryptic\tis_contain\tis_reverse\tis_alternate\tis_init\tis_delete\tis_charade\tis_&lit\tis_hidden\tis_spoonerism\tis_palindrome'.split('\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for df,cc_type in zip(cc_types_dfs,cc_types):\n",
    "    df['category'] = cc_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_val_test(df):\n",
    "    length = len(df)\n",
    "    input_len = math.floor(length*0.7)\n",
    "    val_len  = math.floor(length*0.2)\n",
    "    test_len = math.floor(length*0.1)\n",
    "    input_df = df[:input_len]\n",
    "    val_df = df[input_len:input_len+val_len]\n",
    "    test_df = df[input_len+val_len:]\n",
    "    return input_df,val_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cc_types_df = pd.concat([get_input_val_test(df)[0] for df in cc_types_dfs]).sample(frac=1)\n",
    "val_cc_types_df = pd.concat([get_input_val_test(df)[1] for df in cc_types_dfs]).sample(frac=1)\n",
    "test_cc_types_df = pd.concat([get_input_val_test(df)[2] for df in cc_types_dfs]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = input_cc_types_df.groupby('category').count().max()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [input_cc_types_df]\n",
    "for class_index, group in input_cc_types_df.groupby('category'):\n",
    "    sample = group.sample(max_size-len(group), replace=True, )\n",
    "    lst.append(sample)\n",
    "upsampled_input_cc_types_df = pd.concat(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_input_df = upsampled_input_cc_types_df.drop('category',axis=1)\n",
    "cc_val_df = val_cc_types_df.drop('category',axis=1).drop_duplicates()\n",
    "cc_test_df = test_cc_types_df.drop('category',axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_input_data = pad_sequences(tokenizer.texts_to_sequences(cc_input_df.clue.tolist()),maxlen=15)\n",
    "cc_val_data = pad_sequences(tokenizer.texts_to_sequences(cc_val_df.clue.tolist()),maxlen=15)\n",
    "cc_test_data = pad_sequences(tokenizer.texts_to_sequences(cc_test_df.clue.tolist()),maxlen=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_input_data_out = cc_input_df[cc_input_df.columns[2:]] * 1\n",
    "cc_val_data_out = cc_val_df[cc_val_df.columns[2:]] * 1\n",
    "cc_test_data_out = cc_test_df[cc_test_df.columns[2:]] * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Embedding(len(tokenizer.index_word)+1, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(14, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 149842 samples, validate on 3833 samples\n",
      "Epoch 1/10\n",
      "  6048/149842 [>.............................] - ETA: 4:11 - loss: 0.2643 - categorical_accuracy: 0.1151"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-e0117c6c6c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc_input_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc_input_data_out\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc_val_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcc_val_data_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(cc_input_data,cc_input_data_out ,validation_data=(cc_val_data,cc_val_data_out), batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 255us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2566366582524543, 0.1333333335378591]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=cc_test_data, y=cc_test_data_out, batch_size=None, verbose=1, sample_weight=None, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(len(tokenizer.index_word)+1,50,input_length=15))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(64, 10,padding='valid', activation='relu', strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(250,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(14, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 149842 samples, validate on 3833 samples\n",
      "Epoch 1/6\n",
      "149842/149842 [==============================] - 7s 44us/step - loss: 0.2313 - categorical_accuracy: 0.2141 - val_loss: 0.2445 - val_categorical_accuracy: 0.0744\n",
      "Epoch 2/6\n",
      "149842/149842 [==============================] - 6s 43us/step - loss: 0.2087 - categorical_accuracy: 0.3124 - val_loss: 0.2481 - val_categorical_accuracy: 0.0741\n",
      "Epoch 3/6\n",
      "149842/149842 [==============================] - 6s 42us/step - loss: 0.2002 - categorical_accuracy: 0.3410 - val_loss: 0.2483 - val_categorical_accuracy: 0.0918\n",
      "Epoch 4/6\n",
      "149842/149842 [==============================] - 6s 39us/step - loss: 0.1955 - categorical_accuracy: 0.3582 - val_loss: 0.2490 - val_categorical_accuracy: 0.0822\n",
      "Epoch 5/6\n",
      "149842/149842 [==============================] - 6s 40us/step - loss: 0.1923 - categorical_accuracy: 0.3679 - val_loss: 0.2520 - val_categorical_accuracy: 0.0843\n",
      "Epoch 6/6\n",
      "149842/149842 [==============================] - 6s 40us/step - loss: 0.1901 - categorical_accuracy: 0.3744 - val_loss: 0.2510 - val_categorical_accuracy: 0.0916\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(cc_input_data,cc_input_data_out ,validation_data=(cc_val_data,cc_val_data_out), batch_size=128, epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Embedding(len(tokenizer.index_word)+1, 128))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(14, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"1xBilstm-{epoch:02d}-{val_loss:.2f}-{val_categorical_accuracy:.2f}-singlelabel.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 149842 samples, validate on 3833 samples\n",
      "Epoch 1/16\n",
      "149842/149842 [==============================] - 69s 458us/step - loss: 0.2278 - categorical_accuracy: 0.2305 - val_loss: 0.2482 - val_categorical_accuracy: 0.0764\n",
      "\n",
      "Epoch 00001: saving model to 1xBilstm-01-0.25-0.08-singlelabel.hdf5\n",
      "Epoch 2/16\n",
      "149842/149842 [==============================] - 66s 443us/step - loss: 0.2131 - categorical_accuracy: 0.2964 - val_loss: 0.2412 - val_categorical_accuracy: 0.1135\n",
      "\n",
      "Epoch 00002: saving model to 1xBilstm-02-0.24-0.11-singlelabel.hdf5\n",
      "Epoch 3/16\n",
      "149842/149842 [==============================] - 68s 456us/step - loss: 0.2044 - categorical_accuracy: 0.3277 - val_loss: 0.2520 - val_categorical_accuracy: 0.0897\n",
      "\n",
      "Epoch 00003: saving model to 1xBilstm-03-0.25-0.09-singlelabel.hdf5\n",
      "Epoch 4/16\n",
      "149842/149842 [==============================] - 68s 457us/step - loss: 0.1982 - categorical_accuracy: 0.3500 - val_loss: 0.2533 - val_categorical_accuracy: 0.0871\n",
      "\n",
      "Epoch 00004: saving model to 1xBilstm-04-0.25-0.09-singlelabel.hdf5\n",
      "Epoch 5/16\n",
      "149842/149842 [==============================] - 81s 538us/step - loss: 0.1931 - categorical_accuracy: 0.3689 - val_loss: 0.2478 - val_categorical_accuracy: 0.1036\n",
      "\n",
      "Epoch 00005: saving model to 1xBilstm-05-0.25-0.10-singlelabel.hdf5\n",
      "Epoch 6/16\n",
      "149842/149842 [==============================] - 64s 430us/step - loss: 0.1890 - categorical_accuracy: 0.3819 - val_loss: 0.2486 - val_categorical_accuracy: 0.1070\n",
      "\n",
      "Epoch 00006: saving model to 1xBilstm-06-0.25-0.11-singlelabel.hdf5\n",
      "Epoch 7/16\n",
      "149842/149842 [==============================] - 59s 396us/step - loss: 0.1856 - categorical_accuracy: 0.3941 - val_loss: 0.2499 - val_categorical_accuracy: 0.1145\n",
      "\n",
      "Epoch 00007: saving model to 1xBilstm-07-0.25-0.11-singlelabel.hdf5\n",
      "Epoch 8/16\n",
      "149842/149842 [==============================] - 59s 395us/step - loss: 0.1828 - categorical_accuracy: 0.4036 - val_loss: 0.2517 - val_categorical_accuracy: 0.1064\n",
      "\n",
      "Epoch 00008: saving model to 1xBilstm-08-0.25-0.11-singlelabel.hdf5\n",
      "Epoch 9/16\n",
      "149842/149842 [==============================] - 59s 394us/step - loss: 0.1806 - categorical_accuracy: 0.4106 - val_loss: 0.2516 - val_categorical_accuracy: 0.1098\n",
      "\n",
      "Epoch 00009: saving model to 1xBilstm-09-0.25-0.11-singlelabel.hdf5\n",
      "Epoch 10/16\n",
      "149842/149842 [==============================] - 59s 396us/step - loss: 0.1789 - categorical_accuracy: 0.4160 - val_loss: 0.2524 - val_categorical_accuracy: 0.1054\n",
      "\n",
      "Epoch 00010: saving model to 1xBilstm-10-0.25-0.11-singlelabel.hdf5\n",
      "Epoch 11/16\n",
      "149842/149842 [==============================] - 66s 443us/step - loss: 0.1773 - categorical_accuracy: 0.4221 - val_loss: 0.2569 - val_categorical_accuracy: 0.0999\n",
      "\n",
      "Epoch 00011: saving model to 1xBilstm-11-0.26-0.10-singlelabel.hdf5\n",
      "Epoch 12/16\n",
      "149842/149842 [==============================] - 72s 481us/step - loss: 0.1761 - categorical_accuracy: 0.4245 - val_loss: 0.2514 - val_categorical_accuracy: 0.1117\n",
      "\n",
      "Epoch 00012: saving model to 1xBilstm-12-0.25-0.11-singlelabel.hdf5\n",
      "Epoch 13/16\n",
      "149842/149842 [==============================] - 76s 508us/step - loss: 0.1750 - categorical_accuracy: 0.4290 - val_loss: 0.2580 - val_categorical_accuracy: 0.1015\n",
      "\n",
      "Epoch 00013: saving model to 1xBilstm-13-0.26-0.10-singlelabel.hdf5\n",
      "Epoch 14/16\n",
      "149842/149842 [==============================] - 72s 480us/step - loss: 0.1741 - categorical_accuracy: 0.4316 - val_loss: 0.2549 - val_categorical_accuracy: 0.1096\n",
      "\n",
      "Epoch 00014: saving model to 1xBilstm-14-0.25-0.11-singlelabel.hdf5\n",
      "Epoch 15/16\n",
      "149842/149842 [==============================] - 60s 403us/step - loss: 0.1731 - categorical_accuracy: 0.4346 - val_loss: 0.2558 - val_categorical_accuracy: 0.1104\n",
      "\n",
      "Epoch 00015: saving model to 1xBilstm-15-0.26-0.11-singlelabel.hdf5\n",
      "Epoch 16/16\n",
      "149842/149842 [==============================] - 65s 431us/step - loss: 0.1722 - categorical_accuracy: 0.4373 - val_loss: 0.2639 - val_categorical_accuracy: 0.1114\n",
      "\n",
      "Epoch 00016: saving model to 1xBilstm-16-0.26-0.11-singlelabel.hdf5\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(cc_input_data,cc_input_data_out ,validation_data=(cc_val_data,cc_val_data_out), batch_size=128, epochs=16, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.index_word)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
